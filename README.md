# GNN-Guided Compiler Optimization System

## ðŸŽ¯ Project Overview

This project implements an automated, end-to-end system for predicting the optimal compiler optimization flag (e.g., `-O3`, `-Ofast`, `-Os`) for a given C program. It addresses the "phase ordering" and flag selection problem by leveraging Graph Neural Networks (GNNs) to analyze the intrinsic structure of the code.

Instead of relying on static compiler heuristics, this system:
1.  Extracts the **Control Flow Graph (CFG)** from source code.
2.  Learns structural patterns using a **Graph Convolutional Network (GCN)**.
3.  Predicts the specific optimization flag that will yield the minimum execution time.

## ðŸ“‚ File Manifest

The project directory contains the following key files and folders:

| File / Directory | Description |
| :--- | :--- |
| **`Cprogs/`** | **Input Data:** Directory containing the raw C source code files (`*.c`) used for training and testing. |
| **`FeatureExtractor.cpp`** | **LLVM Pass:** C++ source code for the custom LLVM pass that analyzes bitcode to extract CFG topology and node features (Arithmetic, Loops, Calls). |
| **`build/`** | **Compiled Pass:** Contains the compiled shared library (`FeatureExtractor.so` or `.dylib`) generated by CMake/Make. |
| **`benchmark.py`** | **Ground Truth Pipeline:** Automation script that compiles every program in `Cprogs/` with all target flags (`-O0`...`-Oz`), runs benchmarks, and identifies the fastest flag for each. |
| **`progparser.py`** | **Feature Extraction Pipeline:** Orchestrator script that converts source to Bitcode, invokes the `opt` tool with the custom pass, and parses the raw text output into graph data. |
| **`final_dataset.py`** | **Data Curation:** Script that merges the feature graphs (from `progparser.py`) with the labels (from `benchmark.py`) to create the serialized dataset. |
| **`final_dataset.pt`** | **Ready-to-Train Data:** The processed PyTorch Geometric dataset file containing a list of labeled `Data` objects. |
| **`Train_Test.py`** | **Model Training:** Main script that defines the GNN architecture, loads `final_dataset.pt`, performs training/testing (90/10 split), and evaluates accuracy. |
| **`training_result.png`** | **Visualization:** Generated plot showing Training/Test Accuracy and Loss curves over epochs. |
| **`benchmark_binaries/`** | **Artifacts:** Temporary directory storing the compiled executables generated during the benchmarking phase. |
| **`myenv/`** | **Environment:** Python virtual environment directory containing dependencies. |

## ðŸ§  System Architecture

The system operates via a two-path data generation pipeline feeding into a deep learning model:

1.  **Left Path (Labels):** `Cprogs` $\to$ `benchmark.py` $\to$ Best Flag Labels (Ground Truth).
2.  **Right Path (Features):** `Cprogs` $\to$ `Clang` $\to$ Bitcode $\to$ `FeatureExtractor.cpp` $\to$ `progparser.py` $\to$ Graph Features.
3.  **Convergence:** Both paths merge via `final_dataset.py` into `final_dataset.pt`.
4.  **Training:** The GNN (`Train_Test.py`) learns the mapping between the Graphs and the Labels.


